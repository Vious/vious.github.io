<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content=".">
  <meta name="keywords" content="Video Inpainting, Diffusion models, Training-free">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models
  </title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../static/css/index.css">
  <link rel="icon" href="../static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script src="../static/js/bulma-carousel.min.js"></script>
  <script src="../static/js/bulma-slider.min.js"></script>
  <script src="../static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://vious.github.io/">ChaohaoXie</a>,</span>
            <span class="author-block">
              <a href="https://www.kaihan.org/">Kai Han</a>,</span>
            <span class="author-block">
              <a href="https://i.cs.hku.hk/~kykwong/">Kenneth K.Y. Wong</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">The University of Hong Kong</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Vious/VipDiff"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        <span class="vipdiff">VipDiff</span> provides a training-free solution for taming image-level
        diffusion models to generate temporal-coherent video inpainting results.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose a training-free framework, named VipDiff, for conditioning diffusion model 
            on the reverse diffusion process to produce temporal-coherent inpainting results 
            without requiring any training data or fine-tuning the pre-trained diffusion models. 
          </p>
          <p>
            Our VipDiff takes optical flow as guidance to extract valid pixels from reference 
            frames to serve as constraints in optimizing the randomly sampled Gaussian noise, 
            and uses the generated results for further pixel propagation and conditional generation. 
          </p>
          <p>
            VipDiff also allows for generating diverse video inpainting results over different 
            sampled noise. 
          </p>
        </div>
      </div>
    </div>
<!--     / Abstract. -->
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Video results</h2> -->

        <!-- comparisons. -->
        <h3 class="title is-4">Comparisons with SOTA methods</h3>
        <div class="content has-text-justified">
          <p>
            Our <span class="vipdiff">VipDiff</span> generates spatially and temporally coherent 
            video inpainting results, which surpasses existing state-of-the-art methods 
            on missing areas with large masks.
          </p>
        </div>
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./videos/Comparisons_with_SOTA.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ comparisons. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="./images/Architecture.png"
                 class="Architecture-image"
                 alt="Architecture."/>
          </div>
          Overall framework of our VipDiff. Given a target frame, 
          we first adopt a flow completion model to predict the optical flows. 
          Then the flows are utilized for pixel propagation to extract temporal prior from 
          reference frames to get partially inpainted image. Next, the partially inpainted image
          will act as constrains for optimizing the random sampled Gaussian nosie z 
          which is feed for the reverse denoising U-Net. Through backpropagation, 
          we optimize the noise z at each time step and finally find an optimal z* for 
          filling the target frame. 
          All the model parameters are frozen during the noise optimization process. 
        </div>
      </div>
    </div>
<!--     / Abstract. -->
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
    <!-- Animation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Video results</h2> -->

        <!-- comparisons. -->
        <h3 class="title is-4">Generalization ability</h3>
        <div class="content has-text-justified">
          <p>
            Our <span class="vipdiff">VipDiff</span> can allow different pre-trained image-level 
            diffusion models under different sample strategy to generate diverse results.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="100%">
            <source src="./videos/Generalization_Capability.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ comparisons. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{xie2025vipdiff,
  author    = {Xie, Chaohao and Han, Kai and Wong, Kwan-Yee Kenneth},
  title     = {VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models},
  booktitle   = {IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is constructed using the source code provided by <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, 
            and we are grateful for the template they provided. 
            Allow us to express our appreciation for their contribution.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
